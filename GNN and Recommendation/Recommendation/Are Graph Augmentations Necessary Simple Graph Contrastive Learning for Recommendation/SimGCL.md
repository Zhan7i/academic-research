> 论文标题：Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation
>
> 发表于：SIGIR 2022
>
> 作者：Junliang Yu, Hongzhi Yin,Xin Xia
>
> 代码： https://github.com/Coder-Yu/QRec.
>
> 论文地址：https://arxiv.org/pdf/2112.08679

## 摘要

- 对比学习 (CL) 最近在推荐领域卓有成效，它从原始数据中提取自监督信号的能力与推荐系统解决数据稀疏问题的需求非常吻合。
- 基于 CL 的推荐模型的典型流程是首先用结构扰动扩充用户-项目二部图，然后最大化不同图扩充之间的节点表示一致性。
  - 尽管这种范式被证明是有效的，但性能提升的基础仍然是一个谜。
- 在本文中，在基于 CL 的推荐模型中，CL 通过学习更均匀分布的用户/项目表示来运行，这可以隐式地减轻流行度偏差。
- 揭示了图增强只是起到了微不足道的作用。基于这一发现提出了一种简单的 CL 方法，该方法丢弃了图形增强，而是将均匀噪声添加到嵌入空间以创建对比视图。

## 结论

- 重新审视了推荐中基于 dropout 的 CL，并研究了它如何提高推荐性能。我们发现，在基于 CL 的推荐模型中，CL 损失是核心，图增强仅起次要作用。
- 优化 CL 损失会导致更均匀的表示分布，这有助于在推荐场景中消除偏差。
- 开发了一种简单的无图增强 CL 方法，以更直接的方式调节表示分布的均匀性。通过将定向随机噪声添加到不同数据增强和对比的表示中，所提出的方法可以显着增强推荐。

## 未来工作

## 介绍

- CL 能够从大量未标记数据中提取一般特征并以自我监督的方式对表示进行正则化
- CL 中不需要数据注释，因此它是推荐系统中数据稀疏问题的解决方案
- 将 CL 应用于推荐的典型方法 [29] 是首先使用结构扰动（例如，以一定比例随机边缘/节点丢失）来增强用户-项目二分图，然后通过图形编码器。在此设置中，CL 任务作为辅助任务，与推荐任务联合优化
- 研究通过非 CL 和基于 CL 的推荐方法学习的嵌入空间。通过可视化表示的分布并将它们与其性能相关联，我们发现对推荐性能真正重要的是 CL 损失，而不是图增强
- 优化对比损失 InfoNCE [19] 学习更均匀分布的用户/项目表示，无论是否应用图形增强，这隐含地在减轻流行度偏差 [4] 方面发挥了作用。
- 生成手工制作的图增强需要在训练期间不断重建图邻接矩阵，这非常耗时。此外，删除关键边/节点（例如，切割边）可能会将连通图拆分为几个不连通的组件，这可能会使增强图和原始图共享很少的可学习不变性

## 模型架构

## 实验

- ### 研究问题

- ### 数据集

- ### baseline

- ### 超参数设置

- ### 评估指标
## 批量归一化

- 损失出现在最后，后面的层训练较快
- 当神经网络较深时，数据在最底部，损失在最顶层
  - forward从下往上，但是backward是从上往下的，梯度在上边比较大，越往下变得越小
  - 因为上面层梯度较大，因此上面层会不断地更新
  - 下面层梯度较小，因此权重更新较小，底部的层训练较慢
  - 底部层一变化，所有都得跟着变，最后的那些层需要重新学习多次
  - 导致收敛变慢
- 可以在学习底部层的时候避免变化顶部层吗？

## 核心想法

- 固定小批量里的均值和方差： B为固定批量大小
  - $\large \begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_{B} &= \frac{1}{|{B}|} \sum_{\mathbf{i} \in {B}} \mathbf{x_i},\\
    \hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|{B}|} \sum_{\mathbf{i} \in {B}} (\mathbf{x_i} - {\boldsymbol{\mu}}_{{B}})^2 + \epsilon.\end{aligned}\end{split}$
- 然后做额外的调整(可学习的参数)：  γ，β为可学习参数
  - $\huge x_{i+1} = \boldsymbol{\gamma} \odot \frac{\mathbf{x_i} - {\boldsymbol{\mu}}_\mathcal{B}}{{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}$

## 批量归一化层

- 可学习参数γ、β
- 作用在
  - 全连接层和卷积层输出上，作用在激活函数前
  - 全连接层和卷积层输入上
- 对全连接层，作用在特征维，计算均值和方差
- 对于卷积层，作用在通道维（当做是卷积层的特征维），计算均值和方差

## 批量归一化在做什么

- 初期论文是想用于减少内部协变量转移
- 后续有论文指出它可能就是通过每个小批量里加入噪音来控制模型复杂度：
  - $\huge x_{i+1} = \boldsymbol{\gamma} \odot \frac{\mathbf{x_i} - {\boldsymbol{\hat\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}$
  - $\hat \mu_B$是随机偏移，$\hat \sigma_B$是随机缩放
- 因此没必要跟dropout法混合使用

## 总结

- 批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放
- 可以加速收敛速度，但一般不敢比按模型精度
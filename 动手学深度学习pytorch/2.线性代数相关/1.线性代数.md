```python
A.T	#转置

#范数就是向量或者矩阵的一个长度
#L2范数  ||x||2 = 向量元素平方和的平方根
torch.norm(u)

#L1范数  ：||x||1  每个向量元素的绝对值之和
torch.abs(u).sum()

#矩阵的弗罗贝尼乌斯范数(Frobenius norm) 是矩阵元素的平方和的平方根  ：||X||F
torch.norm(torch.ones((4,9)))

#按特定轴求和  
shape：[5,4]  axis:0, 1   #按行：0  按列：1 求和  把某一轴的元素加起来，相当于消除某一维度
axis = 0 , sum: [4]
axis = 1 , sum: [5]
shape:[2,5,4], axis:0,1,2
axis = 1 , sum:[2,4]
axis = 2 ,sum:[2,5]
axis = [1,2] sum:[2]
shape:[2,5,4]  keepdims = True # keepdims 将某一维变成1，而不是消除
axis = 1 , sum:[2,1,4]
axis = 2 ,sum:[2,5,1]
axis = [1,2] sum:[2,1,1]
```

## Q&A

1.转换是否有负面影响，	比如数值变得稀疏

2.为什么深度学习用张量来表示？	依托于统计学，名词趋向于统计学领域

3.copy和clone的区别 ：   copy分为深浅拷贝不一定会复制内存，clone一定复制内存

4.torch不区分行向量和列向量吗？	1维张量一定是一个行向量，列向量是一个矩阵

5.sum(axis=[0,1])怎么求？	先0后1，矩阵求和

6.机器学习的张量就是多维数组，不是数学中的张量

7.稀疏化有什么好的解决方法？

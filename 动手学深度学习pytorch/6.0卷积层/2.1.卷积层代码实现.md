## 互相关运算

- 输入张量`X`和卷积核张量`K`，并返回输出张量`Y`。

  ```python
  import torch
  from torch import nn
  from d2l import torch as d2l
  
  def corr2d(X, K):  #@save
      """计算二维互相关运算"""
      h, w = K.shape
      Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
      for i in range(Y.shape[0]):
          for j in range(Y.shape[1]):
              Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
      return Y
      
  X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
  K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
  corr2d(X, K)   # output： tensor([[19., 25.], [37., 43.]])
  ```

## 卷积层

- 将`weight`和`bias`声明为两个模型参数。前向传播函数调用`corr2d`函数并添加偏置。

- ```python
  class Conv2D(nn.Module):
      def __init__(self, kernel_size):
          super().__init__()
          self.weight = nn.Parameter(torch.rand(kernel_size))
          self.bias = nn.Parameter(torch.zeros(1))
  
      def forward(self, x):
          return corr2d(x, self.weight) + self.bias
  ```

## 图像中目标的边缘检测：通过找到像素变化的位置，来检测图像中不同颜色的边缘。 

- ```python
  # 首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）
  X = torch.ones((6, 8))
  X[:, 2:6] = 0
  X
  ''' output:
  tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
          [1., 1., 0., 0., 0., 0., 1., 1.],
          [1., 1., 0., 0., 0., 0., 1., 1.],
          [1., 1., 0., 0., 0., 0., 1., 1.],
          [1., 1., 0., 0., 0., 0., 1., 1.],
          [1., 1., 0., 0., 0., 0., 1., 1.]])
  '''
  # 构造一个高度为、宽度为的卷积核K, 当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。
  K = torch.tensor([[1.0, -1.0]])
  
  # 对参数X（输入）和K（卷积核）执行互相关运算
  Y = corr2d(X, K)
  Y
  ''' output:
  tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
          [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])
  '''
  # 将输入的二维图像转置，再进行互相关运算。之前检测到的垂直边缘消失了。这个卷积核K只可以检测垂直边缘，无法检测水平边缘
  corr2d(X.t(), K)
  ''' output:
  tensor([[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]])
  '''
  ```

## 学习由`X`生成`Y`的卷积核

- 使用内置的二维卷积层，并忽略偏置

- ```python
  # 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
  conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)
  
  # 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
  # 其中批量大小和通道数都为1
  X = X.reshape((1, 1, 6, 8))
  Y = Y.reshape((1, 1, 6, 7))  # 卷积后的矩阵
  lr = 3e-2  # 学习率
  
  for i in range(10):
      Y_hat = conv2d(X)
      l = (Y_hat - Y) ** 2
      conv2d.zero_grad()
      l.sum().backward()
      # 迭代卷积核
      conv2d.weight.data[:] -= lr * conv2d.weight.grad # 梯度下降
      if (i + 1) % 2 == 0:
          print(f'epoch {i+1}, loss {l.sum():.3f}')
          
  '''
  epoch 2, loss 11.205
  epoch 4, loss 3.162
  epoch 6, loss 1.056
  epoch 8, loss 0.392
  epoch 10, loss 0.154
  '''
  # 在次迭代之后，误差已经降到足够低。现在我们来看看我们所学的卷积核的权重张量
  conv2d.weight.data.reshape((1, 2))
  # output: tensor([[ 0.9475, -1.0273]])  学习到的卷积核权重非常接近我们之前定义的卷积核K
  ```

  s
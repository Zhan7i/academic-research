## 初始化加载数据集和batch

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

## 初始化模型参数

- 实现一个具有单隐藏层的多层感知机，包含256个隐藏单元

  ```python
  num_inputs, num_outputs, num_hiddens = 784, 10, 256
  
  W1 = nn.Parameter(torch.randn(
      num_inputs, num_hiddens, requires_grad=True) * 0.01)   # nn.Parameter 声明是torch的参数
  b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))   # torch.zeros 设置成0
  W2 = nn.Parameter(torch.randn(
      num_hiddens, num_outputs, requires_grad=True) * 0.01)
  b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
  
  params = [W1, b1, W2, b2]
  ```

## 激活函数

- 实现ReLU激活函数， 而不是直接调用内置的`relu`函数

  ```python
  def relu(X):
      a = torch.zeros_like(X)  # 数据类型与X一样，但是元素全为0
      return torch.max(X, a)
  ```

## 模型

- 使用reshape`将每个二维图像转换为一个长度为`num_inputs`的向量

  ```python
  def net(X):
      X = X.reshape((-1, num_inputs))    # -1读取batch_size
      H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
      return (H@W2 + b2)
  ```

## 损失函数

- 使用高级API中的内置函数来计算softmax和交叉熵损失

  ```python
  loss = nn.CrossEntropyLoss(reduction='none')
  # reduction 该参数在新版本中是为了取代size_average和reduce参数的。
  # 它共有三种选项'elementwise_mean'，'sum'和'none'。
  # 'elementwise_mean'为默认情况，表明对N个样本的loss进行求平均之后返回
  # 'sum'指对n个样本的loss求和
  # 'none'表示直接返回n个样本的loss，每一个数对应一个样本的loss
  
  ```

## 训练

- 多层感知机的训练过程与softmax回归的训练过程完全相同，将迭代周期数设置为10，并将学习率设置为0.1

  ```python
  num_epochs, lr = 10, 0.1
  updater = torch.optim.SGD(params, lr=lr)
  d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
  ```
  
   <img src="img/10.9.jpg" alt="10.9" style="zoom:60%;" />




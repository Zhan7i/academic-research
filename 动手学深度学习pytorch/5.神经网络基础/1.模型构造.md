## 层和块

- 多层感知机 。 下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层， 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层

- ```python
  import torch
  from torch import nn
  from torch.nn import functional as F
  
  net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
  
  X = torch.rand(2, 20)
  net(X)
  ''' output:
  tensor([[-0.1216,  0.0153,  0.0546, -0.0989, -0.0582,  0.1448, -0.3097, -0.0478,
           -0.1381,  0.0593],
          [-0.1315,  0.0540,  0.0157, -0.0701, -0.2307,  0.0710, -0.2731, -0.0527,
           -0.2170,  0.1010]], grad_fn=<AddmmBackward0>)
  '''
  ```

## 自定义块

- 将输入数据作为其前向传播函数的参数。

- 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。

- 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。

- 存储和访问前向传播计算所需的参数。

- 根据需要初始化模型参数。

- ```python
  class MLP(nn.Module):
      # 用模型参数声明层。这里，我们声明两个全连接的层
      def __init__(self):
          # 调用MLP的父类Module的构造函数来执行必要的初始化。
          # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
          super().__init__()
          self.hidden = nn.Linear(20, 256)  # 隐藏层
          self.out = nn.Linear(256, 10)  # 输出层
  
      # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
      def forward(self, X):
          # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
          return self.out(F.relu(self.hidden(X)))
      
  # 实例化多层感知机的层，然后再每次调用正向传播函数时，调用这些层
  net = MLP()
  net(X)
  ''' output:
  tensor([[ 0.0097,  0.0207,  0.1453, -0.0685,  0.0505,  0.2176,  0.0180, -0.2566,
            0.1506, -0.0075],
          [ 0.0425, -0.0261,  0.1969,  0.0842,  0.0037,  0.1542, -0.0176, -0.1798,
            0.0179, -0.1200]], grad_fn=<AddmmBackward0>)
  '''
  ```

## 顺序块

- 定义两个关键函数：

  1. 一种将块逐个追加到列表中的函数。
  2. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”

- ```python
  class MySequential(nn.Module):
      def __init__(self, *args):
          super().__init__()
          for idx, module in enumerate(args):
              # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
              # 变量_modules中。module的类型是OrderedDict
              self._modules[str(idx)] = module
  
      def forward(self, X):
          # OrderedDict保证了按照成员添加的顺序遍历它们
          for block in self._modules.values():
              X = block(X)
          return X
      
  net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
  net(X)
  ''' output:
  tensor([[ 0.0978,  0.0979,  0.1417, -0.3019,  0.0232, -0.0246,  0.0881, -0.0908,
           -0.0428, -0.0388],
          [ 0.1624,  0.0251, -0.0085, -0.3342,  0.0645, -0.1956,  0.1111, -0.0802,
           -0.1252,  0.0666]], grad_fn=<AddmmBackward0>)
  '''
  ```

## 在前向传播函数中执行代码

- 希望执行任意的数学运算， 而不是简单地依赖预定义的神经网络层

- ```python
  class FixedHiddenMLP(nn.Module):
      def __init__(self):
          super().__init__()
          # 不计算梯度的随机权重参数。因此其在训练期间保持不变
          self.rand_weight = torch.rand((20, 20), requires_grad=False)
          self.linear = nn.Linear(20, 20)
  
      def forward(self, X):
          X = self.linear(X)
          # 使用创建的常量参数以及relu和mm函数
          X = F.relu(torch.mm(X, self.rand_weight) + 1)
          # 复用全连接层。这相当于两个全连接层共享参数
          X = self.linear(X)
          # 控制流
          while X.abs().sum() > 1:
              X /= 2
          return X.sum()
      
  net = FixedHiddenMLP()
  net(X)  # output： tensor(-0.0431, grad_fn=<SumBackward0>)
  ```

- 混合搭配各种组合块的方法

  - ```python
    class NestMLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                     nn.Linear(64, 32), nn.ReLU())
            self.linear = nn.Linear(32, 16)
    
        def forward(self, X):
            return self.linear(self.net(X))
    
    chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
    chimera(X)  # output：tensor(0.2444, grad_fn=<SumBackward0>)
    ```

## 小结

- 一个块可以由许多层组成；一个块可以由许多块组成。
- 块可以包含代码。
- 块负责大量的内部处理，包括参数初始化和反向传播。
- 层和块的顺序连接由`Sequential`块处理。
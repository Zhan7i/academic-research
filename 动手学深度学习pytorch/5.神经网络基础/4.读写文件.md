## 加载和保存张量

- 可以直接调用`load`和`save`函数分别读写它们

  - ```python
    import torch
    from torch import nn
    from torch.nn import functional as F
    
    # 存储张量
    x = torch.arange(4)
    torch.save(x, 'x-file')
    # 将存储在文件中的数据读回内存
    x2 = torch.load('x-file')
    x2 # output：tensor([0, 1, 2, 3])
    
    # 存储张量列表
    y = torch.zeros(4)
    torch.save([x, y],'x-files')
    x2, y2 = torch.load('x-files')
    (x2, y2)  # output：(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))
    
    # 写入或读取从字符串映射到张量的字典
    mydict = {'x': x, 'y': y}
    torch.save(mydict, 'mydict')
    mydict2 = torch.load('mydict')
    mydict2  # output：{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}
    ```

## 加载和保存模型参数

- ```python
  class MLP(nn.Module):
      def __init__(self):
          super().__init__()
          self.hidden = nn.Linear(20, 256)
          self.output = nn.Linear(256, 10)
  
      def forward(self, x):
          return self.output(F.relu(self.hidden(x)))
  
  net = MLP()
  X = torch.randn(size=(2, 20))
  Y = net(X)
  
  # 将模型的参数存储在一个叫做“mlp.params”的文件中
  torch.save(net.state_dict(), 'mlp.params')
  
  #为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数
  clone = MLP()
  clone.load_state_dict(torch.load('mlp.params'))
  clone.eval()
  ''' output:
  MLP(
    (hidden): Linear(in_features=20, out_features=256, bias=True)
    (output): Linear(in_features=256, out_features=10, bias=True)
  )
  '''
  # 由于两个实例具有相同的模型参数，在输入相同的X时， 两个实例的计算结果应该相同
  Y_clone = clone(X)
  Y_clone == Y
  # output:tensor([[True, True, True, True, True, True, True, True, True, True],
  #               [True, True, True, True, True, True, True, True, True, True]])
  ```
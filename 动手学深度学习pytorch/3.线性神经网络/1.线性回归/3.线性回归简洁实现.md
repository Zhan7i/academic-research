##  生成数据集

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

## 读取数据集

- 将`features`和`labels`作为API的参数传递，并通过数据迭代器指定`batch_size`。 此外，布尔值`is_train`表示是否希望数据迭代器对象在每个迭代周期内打乱数据

```python
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train) # 每次从样本里随机挑选batch_size个样本

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

##  定义模型

- 在PyTorch中，全连接层在`Linear`类中定义。 将两个参数传递到`nn.Linear`中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1

  ```python
  # nn是神经网络的缩写
  from torch import nn
  
  net = nn.Sequential(nn.Linear(2, 1))
  ```

## 初始化模型参数

- 指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零

  ```python
  net[0].weight.data.normal_(0, 0.01)
  net[0].bias.data.fill_(0)
  ```

## 定义损失函数

- 计算均方误差使用的是`MSELoss`类，也称为平方L2范数。 默认情况下，它返回所有样本损失的平均值。

  ```python
  loss = nn.MSELoss()
  ```

## 定义优化算法

- 小批量随机梯度下降算法。 当我们实例化一个`SGD`实例时，我们要指定优化的参数 （可通过`net.parameters()`从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置`lr`值

  ```python
  trainer = torch.optim.SGD(net.parameters(), lr=0.03) 
  # net.parameters() 神经网络需要的所有参数，包括w和b
  ```

## 训练

- 从`net`访问所需的层，然后读取该层的权重和偏置

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()  # 梯度清零
        l.backward()
        trainer.step() # 模型更新
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

#比较生成数据集的真实参数和通过有限数据训练获得的模型参数
w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)
```

## 问题

- GD、SGD如何找到合适的学习率？
  - 找一个对学习率不那么敏感的算法
  - 找一个合理的参数初始化
- batchsize是否会影响最终模型的结果？
  - 过小可能是好的，但大了不行
  - batch-size越小可能收敛越好，随机梯度下降可能会带来噪音
  - 噪音在一定程度上可能是好事，对于深度神经网络，防止过拟合
- 在过拟合和欠拟合的情况下，学习率和批次该如何进行调整？
  - 理论上习率和批次大小不会太影响收敛结果
- 针对batchsize大小的数据集进行网络训练时，网络中的每个参数更新时减去的梯度是batchsize中每个样本对应参数梯度求和后取得平均值？
  - 是的，梯度是线性的，等价于每个样本相加，每个样本求梯度相加然后取均值
- 为什么优化算法都采用梯度下降(一阶导算法)，而不用牛顿法(二阶导算法)？
  - 一阶导是向量，计算相较于二阶导简单
  - 假设统计模型是错的，那优化模型也是错的，一般的机器学习函数求不到最优解
  - 收敛快不快其实不那么关心，关心的是收敛到哪个程度
- detach()函数的作用
  - 所有的运算自会自动加入计算图求梯度，detach可以把不需要求梯度的运算分离开
- data-iter写法每次把所有输入load进去，数据太多内存不够怎么办？
  - 每次读取批量大小的数据进内存即可
- 生成器生成数据有什么优势？
  - 不需要把所有的batch都生成好，每次要一个batch就去run一遍
## 导入数据集，设置批量大小

- 使用Fashion-MNIST数据集，并保持批量大小为256

- ```python
  import torch
  from torch import nn
  from d2l import torch as d2l
  
  batch_size = 256
  train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
  ```

## 初始化模型参数

- softmax回归的输出层是一个全连接层。  只需在`Sequential`中添加一个带有10个输出的全连接层。 

- 同样，在这里`Sequential`并不是必要的， 但它是实现深度模型的基础。 

- 以均值0和标准差0.01随机初始化权重

- ```python
  # PyTorch不会隐式地调整输入的形状。因此，
  # 我们在线性层前定义了展平层（flatten），来调整网络输入的形状 
  # nn.flatten将任何维度的tensor变成2d的tensor（0维保留，剩下的维度展成向量）
  net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))
  
  def init_weights(m):
      if type(m) == nn.Linear:
          nn.init.normal_(m.weight, std=0.01) # 均值为0 方差为0.01
  
  net.apply(init_weights);
  ```

## *重新审视softmax ：关于上下溢出的问题

- softmax函数$\large \hat y_j=softmax(o)_{j} = \frac{exp(o_{j})}{\sum_kexp(o_{k})}$， 其中$\hat y_j$是预测的概率分布。 $o_j$是未规范化的预测 o 的第 j 个元素。 

- 如果 $o_k$ 中的一些数值非常大， 那么$exp⁡(o_k)$可能大于数据类型容许的最大数字，即*上溢*（overflow）。 这将使分母或分子变为`inf`（无穷大）， 最后得到的是0、`inf`或`nan`（不是数字）的$\hat y_j$。 在这些情况下，我们无法得到一个明确定义的交叉熵值。

  - 解决这个问题的一个技巧是： 在继续softmax计算之前，先从所有 $o_k$ 中减去$max(o_k)$。 你可以看到每个$o_k$按常数进行的移动不会改变softmax的返回值：
  - $\huge \hat y_j = \frac{exp(o_{j} - max(o_k))exp(max(o_k))}{\sum_k exp(o_{k}- max(o_k)) exp( max(o_k))} = \frac{exp(o_{j} - max(o_k))}{\sum_k exp(o_{k}- max(o_k))} $ 

- 在减法和规范化步骤之后，可能有些$o_{j} - max(o_k)$具有较大的负值。 由于精度受限，$o_{j} - max(o_k)$将有接近零的值，即下溢（underflow）。

-  这些值可能会四舍五入为零，使$\hat y_j$为零， 并且使得$log⁡(\hat y_j)$的值为`-inf`。 

- 反向传播几步后，我们可能会发现自己面对一屏幕可怕的`nan`结果。

- 尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。 通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。

- 如下面的等式所示，我们避免计算$exp(o_{j} - max(o_k))$， 而可以直接使用$o_{j} - max(o_k)$，因为log⁡(exp⁡(⋅))被抵消了。

  - $\huge log(\hat y_j) = log(\frac{exp(o_{j} - max(o_k))}{\sum_k exp(o_{k}- max(o_k)) }) $

    $\huge= log(exp(o_{j} - max(o_k)))-log(\sum_k exp(o_{k}- max(o_k))) $ 

    $\huge= o_{j} - max(o_k)-log(\sum_k exp(o_{k}- max(o_k))) $

## 定义交叉熵损失函数

- 在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数

  ```python
  loss = nn.CrossEntropyLoss(reduction='none')
  ```

## 优化算法

- 学习率为0.1的小批量随机梯度下降

- ```python
  trainer = torch.optim.SGD(net.parameters(), lr=0.1)
  ```

## 训练

```python
# 调用之前定义的训练函数来训练模型。
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

<img src="img/9.7.png" alt="9.7" style="zoom:80%;" />
## 统计学习中的重要指标--方差和偏差

<img src="./img/bv.png" alt="bv" style="zoom:80%;" />

- 在统计学习中我们通常会使用方差和偏差来衡量一个模型，
  - 偏差：训练到的模型与真实模型之间的区别（图中蓝点与加号之间的距离）；
  - 方差：每次学习的模型之间差别有多大；
- 图中 中间的加号指的是我们要学的真实模型的地方，圆圈是可容忍的区域，蓝色的圆指的是训练的模型得出的结果，蓝色的点的个数代表了所训练模型的个数
  - 如果所有训练的模型都在大圆内 且 与加号离得很近的话，我们可以认为模型有低偏差和低方差；
  - 如果所有训练的模型没有在大圆内，那么可以说它的偏差很大；但是每一个蓝点之间差别没有那么大，就表示说方差是比较小的；
  - 虽然每个蓝点基本都落到了大圆中，但是每个蓝点的距离比较大；这样偏差比较低，但是方差很大；
  - 最差的一个情况是：每个蓝点既不落在大圆内且每次训练的不一样（之间的距离很大）；即方差和偏差都是很大的
- 我们需要的是低偏差和低方差，这样会是比较好的模型，若出现其他三种情况的话，要考虑用其他方法使得方差和偏差都降低

## 在数学上的定义

- D = {$(x_1,y_1),…,(x_n,y_n)$}   y = f(x) + $ \varepsilon$(噪音)

- 假设每次采样都是从有噪声ε的函数f(x)中采样数据用于学习$\widehat f$，通过学习使得$\widehat f$与 真实的f 尽可能的相近

  - 这是个回归问题可以用最小MSE（均方误差）来实现

- 我们学习到之后需要通过 泛化误差 来衡量它；

- 在统计学习中，我们想通过学习来使得模型能泛化到没有学习过的样本，所以我们需要优先优化 $E[(y-\widehat f(x))^2= 偏差^2 + 方差 +噪声^2] $

  -  $E_D[(y-\widehat f(x))^2] = E[(f - E[\widehat f]) + \varepsilon + (\widehat f - E[\widehat f]))^2] $

    =$(f - E[\widehat f])^2 + E[e^2] + E[(\widehat f - E[\widehat f]))^2]$

    - E[f] = f ,  E[$ \varepsilon$] = 0 ,  方差[$ \varepsilon$] = $\sigma ^2$ , $ \varepsilon$ 独立于 $\widehat f$

## 方差-偏差权衡

<img src="./img/bvT.png" alt="bv" style="zoom:80%;" />

- 刚开始模型过于简单可能学不到真实数据所要表达的内容，这时的偏差的平方会很大，随着模型的逐渐复杂，模型可能可以学到所想表达的信息，所以偏差的平方逐渐变小；
- 随着模型变得越来越复杂，能够拟合的东西就越大，这样模型可能会过多的关注于噪音（数据还是那些数据 数据复杂度低），所以方差会变得越来越大；
- 泛化误差 = 数据本身的噪音，但是数据本身没有变化，应该是个常数；但是加上了偏差和方差，最后就会导致最后的泛化误差曲线就会跟图中的蓝线一样
- 跟之前讲过的下图有关，
  - <img src="./img/bvT2.png" alt="bv" style="zoom:80%;" />

- 训练误差往往会跟偏差相关（偏差越小，模型就越容易拟合到数据上）；图上两条线的差距可以说是方差在起作用； 

## 减小偏差、方差、噪声

- **减小偏差**：偏差很大，说明模型复杂度可能不够，可以使用一个模型复杂度高一点的模型
  - 在神经网络中可以 增加层数 增加隐藏层单元个数（宽度）
  - 也可使用【Boosting；Stacking】
- **减小方差**：
  - 方差太大可能代表你的模型过于复杂，我们可以是用一个简单点的模型，
  - 或者是使用正则化（使用L2，L1正则项，限制住每个模型能够学习的范围）；
  - 也可使用【Bagging；Stacking】
- **降低噪声**：在统计学习中，这个是不可以降低的误差，但是在真实的场景，这是来自于数据采集，
  - 可以通过更精确的数据采集，更干净的数据来使得噪声降低
- **集成学习**：使用多个模型来提升性能 【Boosting；Stacking；Bagging 

## 总结

- 在统计学习中，我们可以把泛化误差分解为 偏差、误差和噪声三项
- 集成学习能够将多个模型组合起来来降低偏差和方差 


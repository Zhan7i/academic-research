- 手工提取特征（用人的知识进行） -->  神经网络来提取特征

- 神经网络（可能更懂机器学习）来提取 可能对后面的线性或softmax回归可能会更好一些

- 用神经网络的好处在于 不用费心思去想 提取的数据特征是否会被模型喜欢

- 但是计算量和数量都比手工提取的数量级要大很多

- 可以使用不同神经网络的架构来更有效的提取特征

  - 多层感知机

  - 卷积神经网络（CNN）

  - 循环神经网络（RNN）

  - Transformers（最近兴起的）

### 线性方法 到 多层感知机

- 稠密层（全连接层或线性层）有可学习的参数W， n是输入特征的维度，m是要输出的向量的长度：$W∈R^{m*n},b∈R^m, y =Wx+b∈R^m$

  - 线性回归可以看成是，有1个输出的全连接层
  - softmax回归可以看成是，有m个输出加上softmax操作子

- #### 怎么变成一个多层感知机

  - 想做到非线性的话，可以使用多个全连接层；但是简单的叠加在一起还是线性的，所以要加入非线性的东西在里面，也就是激活函数；

    - $sigmoid(x) = \frac {1}{1+exp(-x)},ReLU(x) = max(x,0) $
    - 它不是一个线性函数
    - 叫做有一层隐藏层的多层感知机

  - 这里有些超参数

    - 需要选用多少个隐藏层
    - 隐藏层的输出大小；

  - code实现

    ```python
    def relu(X): #对于每个元素 和0求max
        a = torch.zeros_like(X)
        return torch.max(X, a)
    '''
    W1 隐藏层的权重   输入的个数 num_inputs 是定好
    num_hiddens是超参数 隐藏层输出的个数，隐藏单元的个数
    randn（） 产生一个均值为0 方差为1 的正态分布
    b1 偏移  就是隐藏单元的个数
    '''
    W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)
    b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
    W2 = nn.Parameter(torch.randn(
        num_hiddens, num_outputs, requires_grad=True) * 0.01)
    b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
    # A @ B :做矩阵乘法
    H = relu(X @ W1 + b1)  
    Y = H @ W2 + b2
    ```

    
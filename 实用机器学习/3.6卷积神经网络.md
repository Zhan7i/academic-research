### 全连接层 过渡到 卷积层

- 用MLP训练ImageNet(300*300的图片有1000个类（输出）)，MLP有1个隐藏层，隐藏层的输出有1万（输入大概为9万输出是一千，随机取了个中间值1万）

  - 这样的话模型就有10亿个参数可供学习。

  - 这是因为全连接层的输出对所有的输入做加权和，而且每个输出的权重是不一样的，即导致学习的参数特别多。 

- 解决方案

  - 看看做图片分类的时候，有什么先验信息可以使用，使得我们设计神经网络的时候，可以将这个先验知识放进去。
  - 在图片中识别一个物体，有两个原则可以使用
    - Translation invariance（做变换时它不会变）：需要识别的目标在一幅图的一个地方换到另一个地方，不会发生太多的变化
    - Locality（本地性）：识别一个物体不需要看比较远的像素（像素以及其周围的像素相关性比较高）

### 卷积层

- Locality（本地性）：在卷积层中，一个输出只使用一个k×k输入的一个窗口 的图片

  - 全连接层是整张图每个像素进行学习；而卷积层是一个在k×k窗口的像素块内对图片进行学习

- Translation invariance（平移不变性）：如果在一个地方有已经学习好的权重，将它移到另一个地方依然能够识别

- 卷积层可学习参数的个数不再与输出和输入的大小相关，只跟窗口大小k相关

  - 只用在k*k的窗口内做加权和，且每个输出都重用这个窗口

- 这个k*k的窗口叫做 卷积核（kernel或kernel weight），通常被训练用于识别图片的一个模式（可以说是图片中的某些特征）

  - 根据数据，模型会选取不一样的模式出来
  - 通常会使用不同的通道（选很多组的数据得到一个多通道的输出），这样可以学习不同类型的模式

- #### 单通道的卷积

  - ```python
    def corr2d(X, K):  #@save
        """
        Compute 2D cross-correlation 交叉相关.
        both input 'X' and weight 'K' are materices
        """
        h, w = K.shape  #h 长 w 宽  构成h*w的窗口选取
        Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
        for i in range(Y.shape[0]):
            for j in range(Y.shape[1]): 
                Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
        return Y	
    ```

- 卷积层对输入的位置是比较敏感的

  - 任何一个物体在输入中移动时，会导致它对应的输出也会移动。

- 因此对位置移动的鲁棒性，提出了pooling layer（池化层或汇聚层）

  - 就是每一次去计算k*k窗口的元素的均值（平均汇聚）或最大值（最大汇聚）

    - 最大汇聚：对卷积输入有一定的k值，对k个像素的偏移没那么敏感
      - 如果在一个输入里，有k个像素之类的平移，卷积核可以抓住那个物体的输出对应的Y也会发生k个像素之内的平移
      - 如果在卷积的输出的平移是小于k的话，在k*k的窗口里，池化层的输入就是卷积层的输出（取最大值）

  - ```python
    #h, w: pooling window height and weight
    #mode :max or avg
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
            for j in range(Y.shape[1]): 
            	if mode == 'max':
                	Y[i, j] = X[i:i + h, j:j + w].max()
                elif mode == 'avg':
                	Y[i, j] = X[i:i + h, j:j + w].mean()
    ```

### 卷积神经网络

- 卷积神经网络就是一个神经网络，将卷积层堆起来，用卷积来抽取图片中的空间信息
  - 不一定要做图片，可以做跟空间信息有关的东西，只要满足本地性和平移不变性
- 激活层用于每一个卷积层之后，卷积层可以看作是特殊的全连接层，所以卷积层也是线性的，需要激活层
- 卷积层对位置比较敏感，可以用池化层来得到对于位置不是很敏感的输出
- 现代化的CNN有更多的结构化信息
  - 核有多大，通道数有多少，层与层之间是怎么连起来的
  - AlexNet, VGG, Inceptions, ResNet, MobileNet，这些模型都会考虑一些设计模式在里面。 
- <img src="../img/cnn.jpg" alt="cnn" style="zoom:50%;" />

我们要求解，一般是可以通过显式解来求的，一般使用随机梯度下降的方法（SGD）

小批量随机梯度下降，是整个机器学习里面，目前来说几乎是唯一的求解方法，虽然线性模型有很多的方法来求解，但是小批量随机梯度下降可以解决出决策树以外的模型。 

### 小批量随机梯度下降（Mini-batch SGD）

- w 是模型的参数，包括线性模型的w(权重)和b(偏移)
- b 表示批量大小
  - 需要自己设计，老师的动手学深度学习的课程里面有讲怎么选
- $η_t$ 表示在时间 t 的学习率
  - 需要我们来设计，不能选太小，会走不动；也不能选太大，可能整个模型就出问题了

- 步骤：

  - 时刻1时随机的取一个 w1 ；
  - 持续时间到模型收敛（发现目标函数在每个样本平均的损失不再下降；或者是其他的指标基本上趋向平衡）
    - 在每一步里，在所有的样本中随机采样1个 $I_t$ 出来， $I_t$  的大小等于 b 
    - 把 $I_t$ 当成是当前的训练样本 去算目标函数，再通过这个损失函数进一步求得 下一步得权重
      - $W_{t+1} = w_t - \eta_t \bigtriangledown_{w_t}\iota(X_{I_t} , y_{I_t}, w_t)$
    - 不断重复上面的两点直至收敛

- 优点：小批量随机梯度下降可以解决出决策树以外的模型

- 缺点：超参数b与η需要自己选

- ```python
  #'features' shape is (n, p), 'label' shape is (p, 1)  特征  标号
  def data_iter(batch_size, features, labels):
      num_examples = len(features)
      indices = list(range(num_examples))
      # The examples are read at random, in no particular order
      random.shuffle(indices)  #随机采样， 打乱一下
      for i in range(0, num_examples, batch_size):  #小批量采样
          batch_indices = torch.tensor(
              indices[i: min(i + batch_size, num_examples)])
          yield features[batch_indices], labels[batch_indices]  #yield与return不同，可以反复调用，一个一个返回出去
  # w是长为p的向量，均值为0，方差为0.01的高斯分布初始化它的值      requires_grad：创建时需要导数
  w = torch.normal(0, 0.01, size=(p,1), requires_grad=True)
  # 偏移取0 
  b = torch.zeros(1, requires_grad=True)
  
  for epoch in range(num_epochs): #扫一遍数据
      for X, y in data_iter(batch_size, features, labels): #在数据中间读取一个随机的小批量
          y_hat= X @ w + b #X：行数为b，列数为p @做矩阵的乘法
          loss = ((y_hat - y)**2 / 2).mean()  #MSE
          loss.backward()  #求导
          for param in [w, b]
             param -= learning_rate * param.grad
              param.grad.zero_()
  ```

  线性模型是把输入进行线性加权和来的得到我们的预测；
  在线性回归中用平均均方误差来作为损失函数；
  在softmax回归中，用交叉熵（cross-entropy）作为损失函数

  - ​	主要用于多类分类的问题

  小批量随机梯度下降用对两个模型进行求解。之后的神经网络也可以用它来求解 




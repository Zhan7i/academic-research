## Bagging- Bootstrap Aggrgrating

- 做bagging的时候，每次训练n个模型（base learners），但是每个模型都是独立并行训练的，在得到n个模型之后
  - 如果是回归问题，会把每一个模型的输出做平均就得到了bagging出来的效果
  - 如果是做分类的话，这样每一个模型都会输出一个类别，然后会用这些输出做投票选最多的（这个叫Majority voting）
  
- 每个模型的训练是通过bootstrap采样得到的训练集上训练的
  - 什么是bootstrap采样？假设训练集有m个样本，每一次训练base learner的时候，随机采样m个样本，每次采样我们会将这个样本放回去，可能有些样本会重复，如果有n个模型要这样训练，就重复n次
  - 大概是有1-1/e ≈63%的概率会被采样到，就是说可能有37%的样本是没有采样出来的，可以用这个来做验证集，这个也叫做 out of bag
  
- **代码实现**

  ```python
  class Bagging:
  	def __init__(self, base_learnier, n_learners):
  		self.learner = [clone(base_learner) for _ in range(n_learners)]
  		
  	def fit(self, X, y):
  		for learner in self.learners:
              # replace = True 执行随机采样
  			examples = np.random.choice(np.arange(len(X)), int(len(X)), replace = True)
  			learner.fit(X.iloc[examples, :], y.iloc[examples])
  			
  	def predict(self, X):
  		preds = [learner.predict(X) for learner in self.learner]
  		return np.array(preds).mean(axis = 0)  # 在第一个维度做均值
  ```
  

## 随机森林

- 随机森林使用决策树来做base learner；

- 使用随机森林时的常用技术，在bootstrap样本时还会每次随机采样一些特征出来，但在这个地方就不会去采样重复的类出来，因为重复的类没有太大的意义；这样做主要的好处是随机采样之后可以避免一定的过拟合，而且能够增加每一棵决策树之间的差异性；

  <img src="img/bagging.png" alt="bagging" style="zoom:80%;" />

- 在曲线图中，我们可以知道，随着learner的数量增加，模型的误差是逐渐减小的。但是泛化误差的曲线不会往上升，这是因为我们降低了方差但没使得偏差更大，这也就改善了泛化误差中三项其中的一项，但没增加另外两项

## Bagging什么时候会变好

- bagging主要下降的是方差，在统计上采样1次和采样n次取平均，它的均值是不会发生变化的就bias是不会发生变化的，唯一下降的是方差，采样的越多，方差相对来说变得越小。
- 方差什么时候下降的比较快，方差比较大的时候下降的效果比较好。

## 不稳定的learner

- 那什么时候方差大呢，方差比较大的模型我们叫做unstable的模型；
- 以回归来举例子，真实的是f ，base learner是h，bagging之后 对每个学到的base learner的预测值取个均值 就会得到预测值$\hat f$；
- 因为期望的平方会小于方差，所以h(x)与f(x)差别很大的时候，bagging的效果比较好
  - $\large \hat f(x) = E[h(x)]$
  - $(f(x) - \hat f(x)) ≤ E[(f(x)-h(x))^2] \Leftrightarrow (E[h(x)]^2 ≤ E[h(x)^2])$
  - 也就是说，在base learner没那么稳定的时候，它对于下降方差的效果会好

- 决策树不是一个稳定的learner，因为数据一旦发生变化，选取的特征然后选取特征的哪个值都会不一样，分支会不一样，故不稳定；
- 线性回归比较稳定，数据的较小的变化，对模型不会有太大的影响 

## 总结

- bagging就是训练多个模型，每个模型就是通过在训练数据中通过bootstrap采样训练而来；
  - bootstrap就是每一次用m个样本，随机在训练数据中采样m个样本，且会放回继续采样

- bagging的主要效果是能够降低方差，特别是当整个用来做bagging的模型是不稳定的模型的时候效果最佳（随机森林）

